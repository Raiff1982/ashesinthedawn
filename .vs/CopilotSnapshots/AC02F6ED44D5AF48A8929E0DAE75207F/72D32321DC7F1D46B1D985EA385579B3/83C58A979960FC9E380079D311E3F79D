#!/usr/bin/env python
"""
Codette AI Unified Server
Combined FastAPI server for CoreLogic Studio DAW integration
Includes both standard endpoints and production-optimized features
"""

import sys
import os
import json
import logging
import asyncio
import time
import traceback
import hashlib
import uuid
from pathlib import Path
from typing import Optional, Dict, Any, List
from datetime import datetime, timezone
from functools import lru_cache
from pydantic import BaseModel

# Load environment variables from .env file
try:
    from dotenv import load_dotenv
    env_file = Path(__file__).parent / '.env'
    if env_file.exists():
        load_dotenv(env_file)
except ImportError:
    pass  # dotenv not installed, fall back to environment variables

from fastapi import FastAPI, HTTPException, WebSocket, WebSocketDisconnect
from fastapi.middleware.cors import CORSMiddleware
import uvicorn

# Try to import Supabase for music knowledge base
try:
    import supabase
    SUPABASE_AVAILABLE = True
except ImportError:
    SUPABASE_AVAILABLE = False
    print("[WARNING] Supabase not installed - install with: pip install supabase")

# Try to import Redis for persistent caching
try:
    import redis
    REDIS_AVAILABLE = True
except ImportError:
    REDIS_AVAILABLE = False
    print("[INFO] Redis not installed - using in-memory cache (install with: pip install redis)")

# Try to import Codette Enhanced System
try:
    from codette_enhanced_responder import (
        get_enhanced_responder,
        UserRating,
        CodetteEnhancedResponder
    )
    ENHANCED_RESPONDER_AVAILABLE = True
    logger_setup = logging.getLogger(__name__)
    logger_setup.info("[✅] Codette Enhanced Responder imported successfully")
except ImportError as e:
    ENHANCED_RESPONDER_AVAILABLE = False
    logger_setup = logging.getLogger(__name__)
    logger_setup.warning(f"[⚠️] Codette Enhanced Responder not available: {e}")

# Setup paths
codette_path = Path(__file__).parent / "codette"
sys.path.insert(0, str(codette_path))
sys.path.insert(0, str(Path(__file__).parent))

# Import genre templates
try:
    from codette_genre_templates import (
        get_genre_suggestions,
        get_available_genres,
        get_genre_characteristics
    )
    GENRE_TEMPLATES_AVAILABLE = True
except ImportError:
    GENRE_TEMPLATES_AVAILABLE = False
    get_genre_suggestions = None  # type: ignore
    get_available_genres = None  # type: ignore
    get_genre_characteristics = None  # type: ignore
    print("[WARNING] Genre templates not available")

# Try to import numpy for audio analysis
try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    np = None  # type: ignore
    NUMPY_AVAILABLE = False
    print("[WARNING] NumPy not available - some analysis features will be limited")

# ============================================================================
# LOGGING SETUP
# ============================================================================

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ============================================================================
# CACHING SYSTEM FOR PERFORMANCE OPTIMIZATION
# ============================================================================

class ContextCache:
    """TTL-based cache for Supabase context retrieval (reduces API calls ~300ms per query)"""
    
    def __init__(self, ttl_seconds: int = 300):
        self.cache: Dict[str, Dict[str, Any]] = {}
        self.ttl = ttl_seconds
        self.timestamps: Dict[str, float] = {}
        
        # Performance metrics
        self.metrics: Dict[str, Any] = {
            "hits": 0,
            "misses": 0,
            "total_requests": 0,
            "total_hit_latency_ms": 0.0,
            "total_miss_latency_ms": 0.0,
            "average_hit_latency_ms": 0.0,
            "average_miss_latency_ms": 0.0,
            "hit_rate_percent": 0.0,
            "started_at": time.time(),
        }
        self.operation_times: Dict[str, List[float]] = {
            "hits": [],
            "misses": []
        }
    
    def get_cache_key(self, message: str, filename: Optional[str]) -> str:
        """Generate cache key from message + filename"""
        key_text = f"{message}:{filename or 'none'}"
        return hashlib.md5(key_text.encode()).hexdigest()
    
    def get(self, message: str, filename: Optional[str]) -> Optional[Dict[str, Any]]:
        """Get cached context if exists and not expired"""
        start_time = time.time()
        key = self.get_cache_key(message, filename)
        self.metrics["total_requests"] += 1
        
        if key not in self.cache:
            # Cache miss
            elapsed_ms = (time.time() - start_time) * 1000
            self.metrics["misses"] += 1
            self.metrics["total_miss_latency_ms"] += elapsed_ms
            self.operation_times["misses"].append(elapsed_ms)
            logger.debug(f"Cache miss for {message[:30]}... ({elapsed_ms:.2f}ms)")
            return None
        
        # Check if expired
        age = time.time() - self.timestamps[key]
        if age > self.ttl:
            del self.cache[key]
            del self.timestamps[key]
            elapsed_ms = (time.time() - start_time) * 1000
            self.metrics["misses"] += 1
            self.metrics["total_miss_latency_ms"] += elapsed_ms
            self.operation_times["misses"].append(elapsed_ms)
            logger.debug(f"Cache expired for {message[:30]}... ({elapsed_ms:.2f}ms)")
            return None
        
        # Cache hit
        elapsed_ms = (time.time() - start_time) * 1000
        self.metrics["hits"] += 1
        self.metrics["total_hit_latency_ms"] += elapsed_ms
        self.operation_times["hits"].append(elapsed_ms)
        logger.debug(f"Cache hit for {message[:30]}... (age: {age:.1f}s, latency: {elapsed_ms:.2f}ms)")
        return self.cache[key]
    
    def set(self, message: str, filename: Optional[str], data: Dict[str, Any]) -> None:
        """Cache context data with timestamp"""
        key = self.get_cache_key(message, filename)
        self.cache[key] = data
        self.timestamps[key] = time.time()
        logger.debug(f"Cached context for {message[:30]}...")
    
    def clear(self) -> None:
        """Clear all cache"""
        self.cache.clear()
        self.timestamps.clear()
        logger.info("Context cache cleared")
    
    def _update_metrics(self) -> None:
        """Update derived metrics"""
        if self.metrics["total_requests"] > 0:
            self.metrics["hit_rate_percent"] = (
                self.metrics["hits"] / self.metrics["total_requests"] * 100
            )
        
        if self.metrics["hits"] > 0:
            self.metrics["average_hit_latency_ms"] = (
                self.metrics["total_hit_latency_ms"] / self.metrics["hits"]
            )
        
        if self.metrics["misses"] > 0:
            self.metrics["average_miss_latency_ms"] = (
                self.metrics["total_miss_latency_ms"] / self.metrics["misses"]
            )
    
    def stats(self) -> Dict[str, Any]:
        """Get comprehensive cache statistics"""
        uptime_seconds = time.time() - self.metrics["started_at"]
        self._update_metrics()
        
        return {
            "entries": len(self.cache),
            "ttl_seconds": self.ttl,
            "hits": self.metrics["hits"],
            "misses": self.metrics["misses"],
            "total_requests": self.metrics["total_requests"],
            "hit_rate_percent": round(self.metrics["hit_rate_percent"], 2),
            "average_hit_latency_ms": round(self.metrics["average_hit_latency_ms"], 2),
            "average_miss_latency_ms": round(self.metrics["average_miss_latency_ms"], 2),
            "total_hit_latency_ms": round(self.metrics["total_hit_latency_ms"], 2),
            "total_miss_latency_ms": round(self.metrics["total_miss_latency_ms"], 2),
            "uptime_seconds": round(uptime_seconds, 1),
        }

context_cache = ContextCache(ttl_seconds=300)

# ============================================================================
# FASTAPI APP SETUP
# ============================================================================

ALLOWED_ORIGINS = ["http://localhost:5173", "http://localhost:3000", "*"]

app = FastAPI(
    title="Codette AI Unified Server",
    description="Combined Codette AI server for CoreLogic Studio DAW",
    version="2.0.0"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=ALLOWED_ORIGINS,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

logger.info("✅ FastAPI app created with CORS enabled")

# ============================================================================
# SUPABASE CLIENT SETUP (WITH PROPER KEY SELECTION & RLS AWARENESS)
# ============================================================================

supabase_client = None
if SUPABASE_AVAILABLE:
    try:
        supabase_url = os.getenv('VITE_SUPABASE_URL')
        
        # Priority: Service Role Key (full access) > Anon Key (limited by RLS)
        supabase_key = os.getenv('SUPABASE_SERVICE_ROLE_KEY')
        key_type = "service role (full access)"
        key_security_level = "🔐 SECURE - Backend use only"
        
        if not supabase_key:
            # Fallback to anon key if service role not available
            supabase_key = os.getenv('VITE_SUPABASE_ANON_KEY')
            key_type = "anon (limited by RLS policies)"
            key_security_level = "⚠️  LIMITED - Some queries may fail"
            logger.warning("⚠️ SECURITY WARNING: Using anon key - RLS policies may block table access")
            logger.warning("   Recommendation: Set SUPABASE_SERVICE_ROLE_KEY in .env for full backend access")
        
        if supabase_url and supabase_key:
            supabase_client = supabase.create_client(supabase_url, supabase_key)
            logger.info(f"✅ Supabase client connected with {key_type}")
            logger.info(f"   {key_security_level}")
        else:
            logger.warning("⚠️ Supabase credentials not found in environment variables")
    except Exception as e:
        logger.warning(f"⚠️ Failed to connect to Supabase: {e}")

# ============================================================================
# HEALTH ENDPOINTS
# ============================================================================

@app.get("/")
async def root():
    """Root endpoint"""
    return {
        "status": "ok",
        "service": "Codette AI Unified Server",
        "version": "2.0.0",
        "docs": "/docs",
    }

@app.get("/health")
async def health():
    """Health check endpoint"""
    try:
        return {
            "status": "healthy",
            "service": "Codette AI Unified Server",
            "timestamp": datetime.now(timezone.utc).isoformat(),
        }
    except Exception as e:
        logger.error(f"ERROR in /health: {e}")
        return {"status": "error", "error": str(e)}

# ============================================================================
# PYDANTIC MODELS
# ============================================================================

class ChatRequest(BaseModel):
    message: str
    perspective: Optional[str] = "mix_engineering"
    daw_context: Optional[Dict[str, Any]] = None

class SuggestionRequest(BaseModel):
    context: Dict[str, Any]
    limit: Optional[int] = 5

class AnalysisRequest(BaseModel):
    track_data: Optional[Dict[str, Any]] = None
    analysis_type: str = "spectrum"

class TransportRequest(BaseModel):
    action: str  # play, stop, pause, resume, seek
    time_seconds: Optional[float] = 0

# ============================================================================
# ANALYSIS ENDPOINTS
# ============================================================================

@app.get("/api/analysis/delay-sync")
async def get_delay_sync(bpm: float = 120.0):
    """Calculate delay sync times for all note divisions"""
    try:
        divisions = {
            "Whole Note": (60000 / bpm) * 4,
            "Half Note": (60000 / bpm) * 2,
            "Quarter Note": 60000 / bpm,
            "Eighth Note": 30000 / bpm,
            "16th Note": 15000 / bpm,
            "Triplet Quarter": (60000 / bpm) * (2/3),
            "Triplet Eighth": (30000 / bpm) * (2/3),
            "Dotted Quarter": (60000 / bpm) * 1.5,
            "Dotted Eighth": (30000 / bpm) * 1.5,
        }
        logger.info(f"Delay sync calculated for {bpm} BPM")
        return {"status": "success", "bpm": bpm, "divisions": divisions, "timestamp": datetime.now(timezone.utc).isoformat()}
    except Exception as e:
        logger.error(f"ERROR in /api/analysis/delay-sync: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/analysis/detect-genre")
async def detect_genre(request: Dict[str, Any]):
    """Detect music genre based on project metadata"""
    try:
        genres = ["Electronic", "Hip-Hop", "Pop", "Rock", "Jazz", "Classical", "Ambient"]
        return {
            "status": "success",
            "detected_genre": genres[0],
            "confidence": 0.75,
            "candidates": genres[:3],
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
    except Exception as e:
        logger.error(f"ERROR in /api/analysis/detect-genre: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/analysis/ear-training")
async def get_ear_training(exercise_type: str = "interval", difficulty: str = "beginner"):
    """Get ear training exercise data"""
    try:
        exercises = {
            "interval": [
                {"name": "Perfect Unison", "semitones": 0, "difficulty": "beginner"},
                {"name": "Minor Second", "semitones": 1, "difficulty": "beginner"},
                {"name": "Major Third", "semitones": 4, "difficulty": "beginner"},
                {"name": "Perfect Fifth", "semitones": 7, "difficulty": "intermediate"},
            ],
            "chord": [
                {"name": "Major Triad", "intervals": [0, 4, 7], "difficulty": "beginner"},
                {"name": "Minor Triad", "intervals": [0, 3, 7], "difficulty": "beginner"},
                {"name": "Dominant 7", "intervals": [0, 4, 7, 10], "difficulty": "intermediate"},
            ]
        }
        result = exercises.get(exercise_type, exercises["interval"])
        filtered = [e for e in result if e.get("difficulty") == difficulty or difficulty == "all"]
        return {
            "status": "success",
            "exercise_type": exercise_type,
            "difficulty": difficulty,
            "exercises": filtered,
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
    def
