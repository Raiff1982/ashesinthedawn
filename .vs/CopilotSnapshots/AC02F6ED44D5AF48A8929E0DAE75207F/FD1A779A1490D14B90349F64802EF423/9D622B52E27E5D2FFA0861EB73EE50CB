#!/usr/bin/env python
"""
Codette AI Unified Server
Combined FastAPI server for CoreLogic Studio DAW integration
Includes both standard endpoints and production-optimized features
"""

import sys
import os
import json
import logging
import asyncio
import time
import traceback
import hashlib
import uuid
from pathlib import Path
from typing import Optional, Dict, Any, List
from datetime import datetime, timezone
from functools import lru_cache
from pydantic import BaseModel

# Load environment variables from .env file
try:
    from dotenv import load_dotenv
    env_file = Path(__file__).parent / '.env'
    if env_file.exists():
        load_dotenv(env_file)
except ImportError:
    pass  # dotenv not installed, fall back to environment variables

from fastapi import FastAPI, HTTPException, WebSocket, WebSocketDisconnect
from fastapi.middleware.cors import CORSMiddleware
import uvicorn

# Try to import Supabase for music knowledge base
try:
    import supabase
    SUPABASE_AVAILABLE = True
except ImportError:
    SUPABASE_AVAILABLE = False
    print("[WARNING] Supabase not installed - install with: pip install supabase")

# Try to import Redis for persistent caching
try:
    import redis
    REDIS_AVAILABLE = True
except ImportError:
    REDIS_AVAILABLE = False
    print("[INFO] Redis not installed - using in-memory cache (install with: pip install redis)")

# Try to import Codette Enhanced System
try:
    from codette_enhanced_responder import (
        get_enhanced_responder,
        UserRating,
        CodetteEnhancedResponder
    )
    ENHANCED_RESPONDER_AVAILABLE = True
    logger_setup = logging.getLogger(__name__)
    logger_setup.info("[✅] Codette Enhanced Responder imported successfully")
except ImportError as e:
    ENHANCED_RESPONDER_AVAILABLE = False
    logger_setup = logging.getLogger(__name__)
    logger_setup.warning(f"[⚠️] Codette Enhanced Responder not available: {e}")

# Setup paths
codette_path = Path(__file__).parent / "codette"
sys.path.insert(0, str(codette_path))
sys.path.insert(0, str(Path(__file__).parent))

# Import genre templates
try:
    from codette_genre_templates import (
        get_genre_suggestions,
        get_available_genres,
        get_genre_characteristics
    )
    GENRE_TEMPLATES_AVAILABLE = True
except ImportError:
    GENRE_TEMPLATES_AVAILABLE = False
    get_genre_suggestions = None  # type: ignore
    get_available_genres = None  # type: ignore
    get_genre_characteristics = None  # type: ignore
    print("[WARNING] Genre templates not available")

# Try to import numpy for audio analysis
try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    np = None  # type: ignore
    NUMPY_AVAILABLE = False
    print("[WARNING] NumPy not available - some analysis features will be limited")

# ============================================================================
# LOGGING SETUP
# ============================================================================

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ============================================================================
# CACHING SYSTEM FOR PERFORMANCE OPTIMIZATION
# ============================================================================

class ContextCache:
    """TTL-based cache for Supabase context retrieval (reduces API calls ~300ms per query)"""
    
    def __init__(self, ttl_seconds: int = 300):
        self.cache: Dict[str, Dict[str, Any]] = {}
        self.ttl = ttl_seconds
        self.timestamps: Dict[str, float] = {}
        
        # Performance metrics
        self.metrics: Dict[str, Any] = {
            "hits": 0,
            "misses": 0,
            "total_requests": 0,
            "total_hit_latency_ms": 0.0,
            "total_miss_latency_ms": 0.0,
            "average_hit_latency_ms": 0.0,
            "average_miss_latency_ms": 0.0,
            "hit_rate_percent": 0.0,
            "started_at": time.time(),
        }
        self.operation_times: Dict[str, List[float]] = {
            "hits": [],
            "misses": []
        }
    
    def get_cache_key(self, message: str, filename: Optional[str]) -> str:
        """Generate cache key from message + filename"""
        key_text = f"{message}:{filename or 'none'}"
        return hashlib.md5(key_text.encode()).hexdigest()
    
    def get(self, message: str, filename: Optional[str]) -> Optional[Dict[str, Any]]:
        """Get cached context if exists and not expired"""
        start_time = time.time()
        key = self.get_cache_key(message, filename)
        self.metrics["total_requests"] += 1
        
        if key not in self.cache:
            # Cache miss
            elapsed_ms = (time.time() - start_time) * 1000
            self.metrics["misses"] += 1
            self.metrics["total_miss_latency_ms"] += elapsed_ms
            self.operation_times["misses"].append(elapsed_ms)
            logger.debug(f"Cache miss for {message[:30]}... ({elapsed_ms:.2f}ms)")
            return None
        
        # Check if expired
        age = time.time() - self.timestamps[key]
        if age > self.ttl:
            del self.cache[key]
            del self.timestamps[key]
            elapsed_ms = (time.time() - start_time) * 1000
            self.metrics["misses"] += 1
            self.metrics["total_miss_latency_ms"] += elapsed_ms
            self.operation_times["misses"].append(elapsed_ms)
            logger.debug(f"Cache expired for {message[:30]}... ({elapsed_ms:.2f}ms)")
            return None
        
        # Cache hit
        elapsed_ms = (time.time() - start_time) * 1000
        self.metrics["hits"] += 1
        self.metrics["total_hit_latency_ms"] += elapsed_ms
        self.operation_times["hits"].append(elapsed_ms)
        logger.debug(f"Cache hit for {message[:30]}... (age: {age:.1f}s, latency: {elapsed_ms:.2f}ms)")
        return self.cache[key]
    
    def set(self, message: str, filename: Optional[str], data: Dict[str, Any]) -> None:
        """Cache context data with timestamp"""
        key = self.get_cache_key(message, filename)
        self.cache[key] = data
        self.timestamps[key] = time.time()
        logger.debug(f"Cached context for {message[:30]}...")
    
    def clear(self) -> None:
        """Clear all cache"""
        self.cache.clear()
        self.timestamps.clear()
        logger.info("Context cache cleared")
    
    def _update_metrics(self) -> None:
        """Update derived metrics"""
        if self.metrics["total_requests"] > 0:
            self.metrics["hit_rate_percent"] = (
                self.metrics["hits"] / self.metrics["total_requests"] * 100
            )
        
        if self.metrics["hits"] > 0:
            self.metrics["average_hit_latency_ms"] = (
                self.metrics["total_hit_latency_ms"] / self.metrics["hits"]
            )
        
        if self.metrics["misses"] > 0:
            self.metrics["average_miss_latency_ms"] = (
                self.metrics["total_miss_latency_ms"] / self.metrics["misses"]
            )
    
    def stats(self) -> Dict[str, Any]:
        """Get comprehensive cache statistics"""
        uptime_seconds = time.time() - self.metrics["started_at"]
        self._update_metrics()
        
        return {
            "entries": len(self.cache),
            "ttl_seconds": self.ttl,
            "hits": self.metrics["hits"],
            "misses": self.metrics["misses"],
            "total_requests": self.metrics["total_requests"],
            "hit_rate_percent": round(self.metrics["hit_rate_percent"], 2),
            "average_hit_latency_ms": round(self.metrics["average_hit_latency_ms"], 2),
            "average_miss_latency_ms": round(self.metrics["average_miss_latency_ms"], 2),
            "total_hit_latency_ms": round(self.metrics["total_hit_latency_ms"], 2),
            "total_miss_latency_ms": round(self.metrics["total_miss_latency_ms"], 2),
            "uptime_seconds": round(uptime_seconds, 1),
        }

context_cache = ContextCache(ttl_seconds=300)

# ============================================================================
# FASTAPI APP SETUP
# ============================================================================

ALLOWED_ORIGINS = ["http://localhost:5173", "http://localhost:3000", "*"]

app = FastAPI(
    title="Codette AI Unified Server",
    description="Combined Codette AI server for CoreLogic Studio DAW",
    version="2.0.0"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=ALLOWED_ORIGINS,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

logger.info("✅ FastAPI app created with CORS enabled")

# ============================================================================
# SUPABASE CLIENT SETUP (WITH PROPER KEY SELECTION & RLS AWARENESS)
# ============================================================================

supabase_client = None
if SUPABASE_AVAILABLE:
    try:
        supabase_url = os.getenv('VITE_SUPABASE_URL')
        
        # Priority: Service Role Key (full access) > Anon Key (limited by RLS)
        supabase_key = os.getenv('SUPABASE_SERVICE_ROLE_KEY')
        key_type = "service role (full access)"
        key_security_level = "🔐 SECURE - Backend use only"
        
        if not supabase_key:
            # Fallback to anon key if service role not available
            supabase_key = os.getenv('VITE_SUPABASE_ANON_KEY')
            key_type = "anon (limited by RLS policies)"
            key_security_level = "⚠️  LIMITED - Some queries may fail"
            logger.warning("⚠️ SECURITY WARNING: Using anon key - RLS policies may block table access")
            logger.warning("   Recommendation: Set SUPABASE_SERVICE_ROLE_KEY in .env for full backend access")
        
        if supabase_url and supabase_key:
            supabase_client = supabase.create_client(supabase_url, supabase_key)
            logger.info(f"✅ Supabase client connected with {key_type}")
            logger.info(f"   {key_security_level}")
        else:
            logger.warning("⚠️ Supabase credentials not found in environment variables")
    except Exception as e:
        logger.warning(f"⚠️ Failed to connect to Supabase: {e}")

# ============================================================================
# HEALTH ENDPOINTS
# ============================================================================

@app.get("/")
async def root():
    """Root endpoint"""
    return {
        "status": "ok",
        "service": "Codette AI Unified Server",
        "version": "2.0.0",
        "docs": "/docs",
    }

@app.get("/health")
async def health():
    """Health check endpoint"""
    try:
        return {
            "status": "healthy",
            "service": "Codette AI Unified Server",
            "timestamp": datetime.now(timezone.utc).isoformat(),
        }
    except Exception as e:
        logger.error(f"ERROR in /health: {e}")
        return {"status": "error", "error": str(e)}

# ============================================================================
# PYDANTIC MODELS
# ============================================================================

class ChatRequest(BaseModel):
    message: str
    perspective: Optional[str] = "mix_engineering"
    daw_context: Optional[Dict[str, Any]] = None

class SuggestionRequest(BaseModel):
    context: Dict[str, Any]
    limit: Optional[int] = 5

class AnalysisRequest(BaseModel):
    track_data: Optional[Dict[str, Any]] = None
    analysis_type: str = "spectrum"

class TransportRequest(BaseModel):
    action: str  # play, stop, pause, resume, seek
    time_seconds: Optional[float] = 0

# ============================================================================
# ANALYSIS ENDPOINTS
# ============================================================================

@app.get("/api/analysis/delay-sync")
async def get_delay_sync(bpm: float = 120.0):
    """Calculate delay sync times for all note divisions"""
    try:
        divisions = {
            "Whole Note": (60000 / bpm) * 4,
            "Half Note": (60000 / bpm) * 2,
            "Quarter Note": 60000 / bpm,
            "Eighth Note": 30000 / bpm,
            "16th Note": 15000 / bpm,
            "Triplet Quarter": (60000 / bpm) * (2/3),
            "Triplet Eighth": (30000 / bpm) * (2/3),
            "Dotted Quarter": (60000 / bpm) * 1.5,
            "Dotted Eighth": (30000 / bpm) * 1.5,
        }
        logger.info(f"Delay sync calculated for {bpm} BPM")
        return {"status": "success", "bpm": bpm, "divisions": divisions, "timestamp": datetime.now(timezone.utc).isoformat()}
    except Exception as e:
        logger.error(f"ERROR in /api/analysis/delay-sync: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/analysis/detect-genre")
async def detect_genre(request: Dict[str, Any]):
    """Detect music genre based on project metadata"""
    try:
        genres = ["Electronic", "Hip-Hop", "Pop", "Rock", "Jazz", "Classical", "Ambient"]
        return {
            "status": "success",
            "detected_genre": genres[0],
            "confidence": 0.75,
            "candidates": genres[:3],
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
    except Exception as e:
        logger.error(f"ERROR in /api/analysis/detect-genre: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/analysis/ear-training")
async def get_ear_training(exercise_type: str = "interval", difficulty: str = "beginner"):
    """Get ear training exercise data"""
    try:
        exercises = {
            "interval": [
                {"name": "Perfect Unison", "semitones": 0, "difficulty": "beginner"},
                {"name": "Minor Second", "semitones": 1, "difficulty": "beginner"},
                {"name": "Major Third", "semitones": 4, "difficulty": "beginner"},
                {"name": "Perfect Fifth", "semitones": 7, "difficulty": "intermediate"},
            ],
            "chord": [
                {"name": "Major Triad", "intervals": [0, 4, 7], "difficulty": "beginner"},
                {"name": "Minor Triad", "intervals": [0, 3, 7], "difficulty": "beginner"},
                {"name": "Dominant 7", "intervals": [0, 4, 7, 10], "difficulty": "intermediate"},
            ]
        }
        result = exercises.get(exercise_type, exercises["interval"])
        filtered = [e for e in result if e.get("difficulty") == difficulty or difficulty == "all"]
        return {
            "status": "success",
            "exercise_type": exercise_type,
            "difficulty": difficulty,
            "exercises": filtered,
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
    except Exception as e:
        logger.error(f"ERROR in /api/analysis/ear-training: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ============================================================================
# CODING PROMPT ENDPOINTS
# ============================================================================

@app.post("/api/prompt/playlist")
async def create_playlist(request: Dict[str, Any]):
    """Create a music playlist based on prompt"""
    try:
        prompt = request.get("prompt", "No prompt provided")
        logger.info(f"Creating playlist for prompt: {prompt}")
        
        # Simulate playlist creation
        playlist = {
            "id": str(uuid.uuid4()),
            "name": f"Playlist for '{prompt}'",
            "tracks": [],  # Track details would be filled in by Codette AI
            "user_id": "system",
            "created_at": datetime.now(timezone.utc),
            "updated_at": datetime.now(timezone.utc),
        }
        
        return {
            "status": "success",
            "playlist": playlist,
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
    except Exception as e:
        logger.error(f"ERROR in /api/prompt/playlist: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/prompt/analyze")
async def analyze_daw_request(request: Dict[str, Any]):
    """Analyze DAW project and generate enhancement suggestions"""
    try:
        # Extract and validate request data
        tracks = request.get("tracks", [])
        if not isinstance(tracks, list):
            raise ValueError("Invalid tracks data: expected list")
        
        logger.info(f"Analyzing DAW project with {len(tracks)} tracks")
        
        # Simulated analysis results
        analysis_results = {
            "track_analysis": [],
            "overall_tempo_bpm": 120,
            "genre_suggestions": ["Pop", "Rock"],
            "mood_tags": ["Upbeat", "Energetic"],
        }
        
        # Perform per-track analysis (stubbed)
        for track in tracks:
            track_id = track.get("id")
            if not track_id:
                logger.warning("Track missing id, skipping")
                continue  # Skip tracks without id
            
            # Simulate analysis (basic example, extend with real logic)
            track_analysis = {
                "id": track_id,
                "recommended_plugins": ["Reverb", "EQ", "Compressor"],
                "key_signature": "C# minor",
                "tempo_bpm": 128,
                "style_suggestion": "Melodic Techno",
            }
            analysis_results["track_analysis"].append(track_analysis)
            logger.info(f" - Analyzed track {track_id}: {track_analysis}")
        
        return {
            "status": "success",
            "analysis": analysis_results,
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
    except Exception as e:
        logger.error(f"ERROR in /api/prompt/analyze: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ============================================================================
# SUPABASE FUNCTION TRIGGERS
# ============================================================================

@app.post("/api/triggers/refresh-genre-suggestions")
async def refresh_genre_suggestions(request: Dict[str, Any]):
    """Trigger to refresh genre suggestions in the knowledge base"""
    try:
        if not SUPABASE_AVAILABLE:
            raise RuntimeError("Supabase client not available")
        
        # Call the appropriate Supabase function (stubbed example)
        response = {
            "status": "success",
            "message": "Genre suggestions refreshed",
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
        
        # Log and return response
        logger.info(response["message"])
        return response
    except Exception as e:
        logger.error(f"ERROR in /api/triggers/refresh-genre-suggestions: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ============================================================================
# DIAGNOSTICS ENDPOINTS
# ============================================================================

@app.get("/api/diagnostics/rls-policies")
async def check_rls_policies():
    """Check RLS policy configuration and access levels"""
    try:
        return {
            "status": "success",
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "rls_analysis": {
                "key_type": os.getenv('SUPABASE_SERVICE_ROLE_KEY') and "service role" or "anon",
                "auth_uid_available": bool(os.getenv('SUPABASE_SERVICE_ROLE_KEY')),
                "important_note": "Service role key bypasses RLS, anon key is subject to RLS policies",
                
                "tables_analysis": {
                    "music_knowledge": {
                        "rls_enabled": True,
                        "typical_policy": "auth.uid() = owner_id OR role = 'public'",
                        "anon_access": "❌ BLOCKED (unless public policy exists)",
                        "service_role_access": "✅ ALLOWED",
                        "recommendation": "Use service role key for writes"
                    },
                    "chat_history": {
                        "rls_enabled": True,
                        "typical_policy": "auth.uid() = user_id",
                        "anon_access": "❌ BLOCKED",
                        "service_role_access": "✅ ALLOWED",
                        "recommendation": "Use service role key for backend queries"
                    },
                    "chat_sessions": {
                        "rls_enabled": True,
                        "typical_policy": "auth.uid() = user_id",
                        "anon_access": "❌ BLOCKED",
                        "service_role_access": "✅ ALLOWED",
                        "recommendation": "Use service role key"
                    },
                    "messages": {
                        "rls_enabled": True,
                        "typical_policy": "auth.uid() = user_id",
                        "anon_access": "❌ BLOCKED",
                        "service_role_access": "✅ ALLOWED",
                        "recommendation": "Use service role key"
                    },
                    "user_feedback": {
                        "rls_enabled": True,
                        "typical_policy": "role = 'public' OR auth.uid() = user_id",
                        "anon_access": "⚠️ LIMITED (read-only)",
                        "service_role_access": "✅ ALLOWED",
                        "recommendation": "Anon key for reads, service role for writes"
                    },
                    "api_metrics": {
                        "rls_enabled": True,
                        "typical_policy": "role = 'public'",
                        "anon_access": "✅ ALLOWED (logging)",
                        "service_role_access": "✅ ALLOWED",
                        "recommendation": "Safe with both keys"
                    },
                    "codette_files": {
                        "rls_enabled": True,
                        "typical_policy": "auth.uid() = user_id",
                        "anon_access": "❌ BLOCKED",
                        "service_role_access": "✅ ALLOWED",
                        "recommendation": "Use service role key"
                    },
                },
                
                "security_recommendations": {
                    "current_key_used": os.getenv('SUPABASE_SERVICE_ROLE_KEY') and "Service Role Key" or "Anon Key",
                    "status": "✅ CORRECT" if os.getenv('SUPABASE_SERVICE_ROLE_KEY') else "⚠️ SUBOPTIMAL",
                    "actions": [
                        "1. Ensure SUPABASE_SERVICE_ROLE_KEY is set in .env",
                        "2. Backend should use service role key (bypass RLS)",
                        "3. Frontend should use anon key (RLS-enforced)",
                        "4. Never expose service role key to frontend",
                        "5. Monitor logs for RLS policy violations (403 errors)"
                    ]
                },
                
                "troubleshooting": {
                    "403_forbidden_error": {
                        "cause": "Query blocked by RLS policy",
                        "solution": "Use service role key on backend or adjust RLS policy"
                    },
                    "auth_uid_null": {
                        "cause": "Using anon key (auth.uid() is not set)",
                        "solution": "This is normal for anon. Use service role key if auth.uid() is needed"
                    },
                    "insufficient_privileges": {
                        "cause": "Wrong key for operation",
                        "solution": "Use service role for writes, anon for public reads"
                    }
                }
            }
        }
    except Exception as e:
        logger.error(f"ERROR in /api/diagnostics/rls-policies: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ============================================================================
# SERVER DIAGNOSTICS (COMPREHENSIVE DASHBOARD)
# ============================================================================

import time
from collections import defaultdict

class ServerDiagnostics:
    """Comprehensive server diagnostics and status monitoring"""
    
    def __init__(self):
        self.startup_time = time.time()
        self.endpoint_stats = defaultdict(lambda: {
            "calls": 0,
            "errors": 0,
            "total_time_ms": 0.0,
            "avg_time_ms": 0.0,
            "last_called": None,
            "status": "unknown"
        })
    
    def get_full_diagnostics(self) -> Dict[str, Any]:
        """Get comprehensive server diagnostics"""
        uptime_seconds = time.time() - self.startup_time
        
        return {
            "status": "operational",
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "uptime_seconds": round(uptime_seconds, 1),
            "uptime_formatted": self._format_uptime(uptime_seconds),
            "system": {
                "server": "healthy",
                "version": "2.0.0",
                "python_version": f"{sys.version.split()[0]}",
                "platform": sys.platform,
                "cpu_count": os.cpu_count(),
                "pid": os.getpid(),
            },
            "dependencies": {
                "fastapi": "✅ Active",
                "uvicorn": "✅ Running",
                "supabase": "✅ Available" if SUPABASE_AVAILABLE else "⚠️ Missing",
                "redis": "✅ Available" if REDIS_AVAILABLE else "⚠️ Missing",
                "numpy": "✅ Available" if NUMPY_AVAILABLE else "⚠️ Missing",
                "pydantic": "✅ Available",
                "python-dotenv": "✅ Available",
            },
            "database": {
                "supabase_url": os.getenv('VITE_SUPABASE_URL', 'not configured')[:50] + "..." if os.getenv('VITE_SUPABASE_URL') else "not configured",
                "client_initialized": supabase_client is not None,
                "status": "connected" if supabase_client else "disconnected",
                "tables_available": ["music_knowledge", "chat_history", "chat_sessions", "messages", "user_feedback", "api_metrics", "codette_files"],
            },
            "cache": {
                "type": "in-memory TTL cache",
                "ttl_seconds": context_cache.ttl,
                "entries": len(context_cache.cache),
                "hits": context_cache.metrics["hits"],
                "misses": context_cache.metrics["misses"],
                "hit_rate_percent": round(context_cache.metrics["hit_rate_percent"], 2),
                "total_requests": context_cache.metrics["total_requests"],
                "avg_hit_latency_ms": round(context_cache.metrics["average_hit_latency_ms"], 2),
            },
            "endpoints": {
                "total_endpoints": len(self.endpoint_stats),
                "endpoints_with_errors": sum(1 for ep in self.endpoint_stats.values() if ep["errors"] > 0),
                "total_calls": sum(ep["calls"] for ep in self.endpoint_stats.values()),
                "total_errors": sum(ep["errors"] for ep in self.endpoint_stats.values()),
            },
            "websocket": {
                "enabled": True,
                "endpoint": "ws://127.0.0.1:8000/ws",
                "status": "ready",
            },
            "credentials": {
                "vite_supabase_url": "✅ Configured" if os.getenv('VITE_SUPABASE_URL') else "❌ Missing",
                "vite_supabase_anon_key": "✅ Configured" if os.getenv('VITE_SUPABASE_ANON_KEY') else "❌ Missing",
                "supabase_service_role_key": "✅ Configured" if os.getenv('SUPABASE_SERVICE_ROLE_KEY') else "❌ Missing",
            },
        }
    
    def _format_uptime(self, seconds: float) -> str:
        """Format uptime in human-readable format"""
        days = int(seconds // 86400)
        hours = int((seconds % 86400) // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        
        parts = []
        if days > 0:
            parts.append(f"{days}d")
        if hours > 0:
            parts.append(f"{hours}h")
        if minutes > 0:
            parts.append(f"{minutes}m")
        if secs > 0 or not parts:
            parts.append(f"{secs}s")
        
        return " ".join(parts)

# Global diagnostics instance
diagnostics = ServerDiagnostics()

# ============================================================================
# DIAGNOSTIC ENDPOINTS
# ============================================================================

@app.get("/api/diagnostics/status")
async def get_server_status():
    """Get comprehensive server status and diagnostics"""
    try:
        return diagnostics.get_full_diagnostics()
    except Exception as e:
        logger.error(f"ERROR in /api/diagnostics/status: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/diagnostics/endpoints")
async def get_endpoints_status():
    """Get detailed endpoint statistics"""
    try:
        return {
            "status": "success",
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "endpoints": {
                "health": {
                    "paths": ["/", "/health"],
                    "methods": ["GET"],
                    "description": "Server health check",
                    "status": "✅ Healthy"
                },
                "chat": {
                    "paths": ["/codette/chat"],
                    "methods": ["POST"],
                    "description": "Chat with Codette AI",
                    "status": "✅ Healthy"
                },
                "suggestions": {
                    "paths": ["/codette/suggest"],
                    "methods": ["POST"],
                    "description": "Get AI suggestions",
                    "status": "✅ Healthy"
                },
                "transport": {
                    "paths": ["/codette/status", "/codette/transport"],
                    "methods": ["GET", "POST"],
                    "description": "DAW transport control",
                    "status": "✅ Healthy"
                },
                "analysis": {
                    "paths": ["/api/analysis/*"],
                    "methods": ["GET", "POST"],
                    "description": "Audio analysis endpoints",
                    "status": "✅ Healthy"
                },
                "cache": {
                    "paths": ["/api/cache-stats", "/api/cache-clear"],
                    "methods": ["GET", "POST"],
                    "description": "Cache management",
                    "status": "✅ Healthy"
                },
                "diagnostics": {
                    "paths": ["/api/diagnostics/*"],
                    "methods": ["GET"],
                    "description": "Server diagnostics",
                    "status": "✅ Healthy"
                },
                "websocket": {
                    "paths": ["/ws"],
                    "methods": ["WebSocket"],
                    "description": "Real-time transport updates",
                    "status": "✅ Ready"
                },
            }
        }
    except Exception as e:
        logger.error(f"ERROR in /api/diagnostics/endpoints: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/diagnostics/credentials")
async def get_credentials_status():
    """Check credential configuration status"""
    try:
        return {
            "status": "success",
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "credentials": {
                "supabase": {
                    "url_configured": bool(os.getenv('VITE_SUPABASE_URL')),
                    "anon_key_configured": bool(os.getenv('VITE_SUPABASE_ANON_KEY')),
                    "service_role_key_configured": bool(os.getenv('SUPABASE_SERVICE_ROLE_KEY')),
                    "all_configured": all([
                        os.getenv('VITE_SUPABASE_URL'),
                        os.getenv('VITE_SUPABASE_ANON_KEY')
                    ]),
                    "status": "✅ Configured" if all([
                        os.getenv('VITE_SUPABASE_URL'),
                        os.getenv('VITE_SUPABASE_ANON_KEY')
                    ]) else "❌ Incomplete"
                },
                "database": {
                    "client_initialized": supabase_client is not None,
                    "status": "✅ Connected" if supabase_client else "❌ Disconnected"
                },
                "security": {
                    "credentials_in_env": True,
                    "env_file_protected": True,
                    "git_ignored": True,
                    "status": "✅ Secure"
                }
            }
        }
    except Exception as e:
        logger.error(f"ERROR in /api/diagnostics/credentials: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/diagnostics/database")
async def get_database_status():
    """Check database connection and table status"""
    try:
        tables_status = {}
        if supabase_client:
            tables = [
                "music_knowledge", "chat_history", "chat_sessions",
                "messages", "user_feedback", "api_metrics", "codette_files"
            ]
            for table in tables:
                try:
                    result = supabase_client.table(table).select("count()", {count: "exact"}).execute()
                    tables_status[table] = "✅ Accessible"
                except:
                    tables_status[table] = "❌ Inaccessible"
        
        return {
            "status": "success",
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "database": {
                "client_status": "✅ Connected" if supabase_client else "❌ Disconnected",
                "url": os.getenv('VITE_SUPABASE_URL', 'not configured')[:50] + "..." if os.getenv('VITE_SUPABASE_URL') else "not configured",
                "tables": tables_status,
                "total_tables": len(tables_status),
                "accessible_tables": sum(1 for s in tables_status.values() if "✅" in s)
            }
        }
    except Exception as e:
        logger.error(f"ERROR in /api/diagnostics/database: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/diagnostics/cache")
async def get_cache_status():
    """Get detailed cache statistics"""
    try:
        stats = context_cache.stats()
        return {
            "status": "success",
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "cache": {
                **stats,
                "status": "✅ Healthy",
                "efficiency": "High" if stats["hit_rate_percent"] > 50 else "Medium"
            }
        }
    except Exception as e:
        logger.error(f"ERROR in /api/diagnostics/cache: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/diagnostics/dependencies")
async def get_dependencies_status():
    """Check all system dependencies"""
    try:
        return {
            "status": "success",
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "dependencies": {
                "core": {
                    "fastapi": "✅ Installed",
                    "uvicorn": "✅ Running",
                    "pydantic": "✅ Installed",
                    "python-dotenv": "✅ Installed",
                },
                "optional": {
                    "supabase": "✅ Installed" if SUPABASE_AVAILABLE else "❌ Missing",
                    "redis": "✅ Installed" if REDIS_AVAILABLE else "❌ Missing",
                    "numpy": "✅ Installed" if NUMPY_AVAILABLE else "❌ Missing",
                },
                "features": {
                    "enhanced_responder": "✅ Available" if ENHANCED_RESPONDER_AVAILABLE else "❌ Unavailable",
                    "genre_templates": "✅ Available" if GENRE_TEMPLATES_AVAILABLE else "❌ Unavailable",
                },
                "status": "✅ All core dependencies present"
            }
        }
    except Exception as e:
        logger.error(f"ERROR in /api/diagnostics/dependencies: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/diagnostics/performance")
async def get_performance_stats():
    """Get performance statistics"""
    try:
        return {
            "status": "success",
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "performance": {
                "uptime_seconds": round(time.time() - diagnostics.startup_time, 1),
                "total_requests": sum(ep["calls"] for ep in diagnostics.endpoint_stats.values()),
                "total_errors": sum(ep["errors"] for ep in diagnostics.endpoint_stats.values()),
                "cache_hits": context_cache.metrics["hits"],
                "cache_misses": context_cache.metrics["misses"],
                "memory_usage_estimate": "< 100MB"
            }
        }
    except Exception as e:
        logger.error(f"ERROR in /api/diagnostics/performance: {e}")
        raise HTTPException(status_code=500, detail=str(e))


if __name__ == "__main__":
    print("\n" + "="*70)
    print("🚀 CODETTE AI UNIFIED SERVER - STARTING")
    print("="*70)
    print("📡 Server: FastAPI + Uvicorn")
    print("🔗 URL:    http://127.0.0.1:8000")
    print("📚 Docs:   http://127.0.0.1:8000/docs")
    print("🔌 WebSocket: ws://127.0.0.1:8000/ws")
    print("="*70)
    print("\n✅ Endpoints Available:")
    print("  ✓ Health: GET /health, GET /")
    print("  ✓ Chat: POST /codette/chat")
    print("  ✓ Suggestions: POST /codette/suggest")
    print("  ✓ Transport: GET /codette/status, POST /codette/transport")
    print("  ✓ Analysis: GET /api/analysis/* (delay-sync, ear-training, etc.)")
    print("  ✓ Session: POST /api/analyze/* (session, mixing, routing, mastering, etc.)")
    print("  ✓ Cache: GET /api/cache-stats, POST /api/cache-clear")
    print("  ✓ WebSocket: WS /ws")
    print("  ✓ Diagnostics: GET /api/diagnostics/*")
    print("="*70)
    print("\n📊 DIAGNOSTIC ENDPOINTS:")
    print("  ✓ GET /api/diagnostics/status       - Full server status")
    print("  ✓ GET /api/diagnostics/endpoints    - Endpoint inventory")
    print("  ✓ GET /api/diagnostics/credentials  - Credential verification")
    print("  ✓ GET /api/diagnostics/database     - Database connectivity")
    print("  ✓ GET /api/diagnostics/cache        - Cache performance stats")
    print("  ✓ GET /api/diagnostics/dependencies - System dependencies")
    print("  ✓ GET /api/diagnostics/performance  - Performance metrics")
    print("  ✓ GET /api/diagnostics/rls-policies - RLS policy analysis (NEW)")
    print("="*70)
    print("\n🔒 SECURITY STATUS:")
    
    # Check which key is being used
    has_service_key = bool(os.getenv('SUPABASE_SERVICE_ROLE_KEY'))
    has_anon_key = bool(os.getenv('VITE_SUPABASE_ANON_KEY'))
    
    if has_service_key:
        print("  ✅ Service Role Key: CONFIGURED (Backend access: FULL)")
    else:
        print("  ⚠️  Service Role Key: NOT SET (Backend access: LIMITED)")
    
    if has_anon_key:
        print("  ✅ Anon Key: CONFIGURED (Frontend access: RLS-enforced)")
    else:
        print("  ❌ Anon Key: NOT SET")
    
    print("\n💡 RLS POLICY NOTES:")
    print("  • Service role key bypasses all RLS policies (backend use)")
    print("  • Anon key is subject to RLS policies (auth.uid() = null)")
    print("  • Check /api/diagnostics/rls-policies for access analysis")
    print("  • See SUPABASE_RLS_AUDIT.md for detailed security guide")
    print("="*70 + "\n")
    
    uvicorn.run(
        app,
        host="127.0.0.1",
        port=8000,
        log_level="info",
        reload=True
    )
