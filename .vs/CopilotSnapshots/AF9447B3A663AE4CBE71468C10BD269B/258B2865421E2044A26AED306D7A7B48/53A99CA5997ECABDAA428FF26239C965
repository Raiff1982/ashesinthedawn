import logging
import nltk
import numpy as np
import sympy as sp
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
import re
from typing import List, Dict, Any, Optional
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.tag import pos_tag
from nltk.corpus import wordnet
import random
import os
from datetime import datetime

logger = logging.getLogger(__name__)

# Try to import Supabase (optional dependency)
try:
    from supabase import create_client, Client
    SUPABASE_AVAILABLE = True
except ImportError:
    SUPABASE_AVAILABLE = False
    logger.warning("Supabase not available - install with: pip install supabase")

# Download required NLTK data with error handling
try:
    nltk.download('punkt', quiet=True)
    nltk.download('averaged_perceptron_tagger', quiet=True)
    nltk.download('wordnet', quiet=True)
except Exception as e:
    logger.warning(f"NLTK download failed (this is non-critical): {e}")

class Codette:
    def __init__(self, user_name="User"):
        self.user_name = user_name
        self.memory = []
        self.analyzer = SentimentIntensityAnalyzer()
        np.seterr(divide='ignore', invalid='ignore')
        self.audit_log("Codette initialized", system=True)
        self.context_memory = []
        
        # DAW-specific knowledge base
        self.daw_knowledge = self._initialize_daw_knowledge()
        
        # Response variation tracking to prevent repetition
        self.recent_responses = []
        self.max_recent_responses = 20
        
        # Personality modes for response variation
        self.personality_modes = [
            'technical_expert',     # Precise, technical, professional
            'creative_mentor',      # Inspirational, metaphorical, encouraging
            'practical_guide',      # Direct, actionable, efficient
            'analytical_teacher',   # Detailed, explanatory, educational
            'innovative_explorer'   # Experimental, cutting-edge, forward-thinking
        ]
        self.current_personality = 'technical_expert'
        
        # Initialize Supabase client (if available)
        self.supabase_client = self._initialize_supabase()
        
        # Conversation context (recent topics)
        self.conversation_topics = []
        self.max_conversation_topics = 10
    
    def _initialize_supabase(self) -> Optional[Any]:
        """Initialize Supabase client for persistent storage"""
        if not SUPABASE_AVAILABLE:
            logger.info("Codette running without Supabase integration")
            return None
        
        try:
            supabase_url = os.getenv('VITE_SUPABASE_URL')
            supabase_key = os.getenv('SUPABASE_SERVICE_ROLE_KEY') or os.getenv('VITE_SUPABASE_ANON_KEY')
            
            if supabase_url and supabase_key:
                client = create_client(supabase_url, supabase_key)
                logger.info("✅ Codette connected to Supabase for persistent knowledge")
                return client
            else:
                logger.info("Codette running without Supabase (credentials not configured)")
                return None
        except Exception as e:
            logger.warning(f"Codette Supabase init failed (non-critical): {e}")
            return None
    
    def save_conversation_to_db(self, prompt: str, response: str, metadata: Optional[Dict[str, Any]] = None):
        """Save conversation to Supabase for learning and context"""
        if not self.supabase_client:
            return
        
        try:
            conversation_entry = {
                'user_name': self.user_name,
                'prompt': prompt,
                'response': response,
                'personality_mode': self.current_personality,
                'metadata': metadata or {},
                'created_at': datetime.utcnow().isoformat()
            }
            
            # Insert into codette_conversations table
            self.supabase_client.table('codette_conversations').insert(conversation_entry).execute()
            logger.debug(f"Saved conversation to Supabase: {prompt[:50]}...")
        except Exception as e:
            logger.debug(f"Failed to save conversation (non-critical): {e}")
    
    def get_conversation_history(self, limit: int = 10) -> List[Dict[str, Any]]:
        """Retrieve recent conversation history from Supabase"""
        if not self.supabase_client:
            return []
        
        try:
            response = self.supabase_client.table('codette_conversations') \
                .select('*') \
                .eq('user_name', self.user_name) \
                .order('created_at', desc=True) \
                .limit(limit) \
                .execute()
            
            return response.data if response.data else []
        except Exception as e:
            logger.debug(f"Failed to retrieve conversation history: {e}")
            return []
    
    def rotate_personality(self):
        """Rotate personality mode for response variation"""
        current_index = self.personality_modes.index(self.current_personality)
        next_index = (current_index + 1) % len(self.personality_modes)
        self.current_personality = self.personality_modes[next_index]
        logger.debug(f"Codette personality rotated to: {self.current_personality}")
    
    def get_personality_prefix(self) -> str:
        """Get response prefix based on personality mode"""
        prefixes = {
            'technical_expert': '[Technical Expert]',
            'creative_mentor': '[Creative Mentor]',
            'practical_guide': '[Practical Guide]',
            'analytical_teacher': '[Audio Engineer]',
            'innovative_explorer': '[Innovation Lab]'
        }
        return prefixes.get(self.current_personality, '[DAW Expert]')
        
    def _initialize_daw_knowledge(self) -> Dict[str, Any]:
        """Initialize DAW-specific knowledge base for intelligent suggestions"""
        return {
            'frequency_ranges': {
                'sub_bass': {'min': 20, 'max': 60, 'desc': 'Physical punch and power'},
                'bass': {'min': 60, 'max': 250, 'desc': 'Fundamental low-end warmth'},
                'low_mids': {'min': 250, 'max': 500, 'desc': 'Body and fullness'},
                'mids': {'min': 500, 'max': 2000, 'desc': 'Core tonal character'},
                'upper_mids': {'min': 2000, 'max': 4000, 'desc': 'Presence and definition'},
                'highs': {'min': 4000, 'max': 8000, 'desc': 'Clarity and articulation'},
                'air': {'min': 8000, 'max': 20000, 'desc': 'Sparkle and openness'}
            },
            'track_types': {
                'audio': {
                    'common_issues': ['phase problems', 'muddy low-end', 'harsh highs', 'lack of presence'],
                    'mixing_tips': [
                        'Use high-pass filter to remove unnecessary low frequencies',
                        'Apply gentle EQ to enhance the natural tone',
                        'Use compression to control dynamics',
                        'Add subtle reverb for spatial depth'
                    ]
                },
                'instrument': {
                    'common_issues': ['timing inconsistencies', 'velocity variations', 'quantization artifacts'],
                    'mixing_tips': [
                        'Humanize MIDI velocity for natural feel',
                        'Layer multiple synth patches for richness',
                        'Use sidechain compression with kick/bass',
                        'Apply stereo widening carefully'
                    ]
                },
                'vocals': {
                    'common_issues': ['sibilance', 'breath noise', 'inconsistent volume', 'lack of air'],
                    'mixing_tips': [
                        'De-ess harsh frequencies (6-8kHz)',
                        'Boost presence around 3-5kHz',
                        'Add air with gentle 10-12kHz boost',
                        'Use compression (3:1 to 6:1 ratio)',
                        'Double-track for width and depth'
                    ]
                },
                'drums': {
                    'common_issues': ['weak kick', 'thin snare', 'muddy mix', 'lack of punch'],
                    'mixing_tips': [
                        'High-pass individual drums except kick and snare',
                        'Boost kick fundamental (50-100Hz)',
                        'Add snare crack at 3-5kHz',
                        'Use parallel compression for power',
                        'Pan toms and cymbals for stereo width'
                    ]
                }
            },
            'mixing_principles': {
                'gain_staging': {
                    'target_peak': -6,  # dB
                    'target_rms': -18,  # dB
                    'headroom': 6,  # dB for mastering
                    'description': 'Proper gain staging prevents clipping and maintains headroom'
                },
                'eq_guidelines': {
                    'cut_before_boost': True,
                    'narrow_q_for_cuts': True,
                    'wide_q_for_boosts': True,
                    'description': 'Subtractive EQ is more transparent than additive'
                },
                'compression': {
                    'attack_fast': {'range': [1, 10], 'unit': 'ms', 'use': 'control transients'},
                    'attack_slow': {'range': [20, 50], 'unit': 'ms', 'use': 'preserve transients'},
                    'release_fast': {'range': [50, 150], 'unit': 'ms', 'use': 'pumping effect'},
                    'release_auto': {'description': 'adapts to signal, most transparent'},
                    'ratio_light': {'range': [2, 3], 'use': 'subtle control'},
                    'ratio_heavy': {'range': [6, 10], 'use': 'aggressive control'}
                },
                'panning': {
                    'center': ['kick', 'snare', 'bass', 'lead_vocal'],
                    'wide': ['guitars', 'backing_vocals', 'synth_pads'],
                    'extreme': ['percussion', 'effects', 'ear_candy']
                }
            },
            'genre_characteristics': {
                'electronic': {
                    'key_elements': ['tight low-end', 'wide synths', 'punchy drums', 'automated filters'],
                    'typical_bpm': [120, 140],
                    'mixing_focus': 'clarity in high-energy sections'
                },
                'hip_hop': {
                    'key_elements': ['dominant bass', 'crisp drums', 'clear vocals', 'rhythmic elements'],
                    'typical_bpm': [80, 100],
                    'mixing_focus': 'vocal presence and low-end power'
                },
                'rock': {
                    'key_elements': ['distorted guitars', 'live drums', 'dynamic range', 'raw energy'],
                    'typical_bpm': [100, 140],
                    'mixing_focus': 'balance between power and clarity'
                },
                'pop': {
                    'key_elements': ['polished vocals', 'layered instruments', 'radio-ready sound', 'hooks'],
                    'typical_bpm': [100, 130],
                    'mixing_focus': 'commercial loudness and clarity'
                }
            },
            'common_problems': {
                'muddy_mix': {
                    'causes': ['low-mid buildup', 'too much reverb', 'overlapping frequencies'],
                    'solutions': [
                        'High-pass filter non-bass instruments above 100Hz',
                        'Cut 200-400Hz on multiple tracks',
                        'Use shorter reverb times',
                        'Create frequency separation between instruments'
                    ]
                },
                'harsh_highs': {
                    'causes': ['excessive 3-5kHz', 'digital clipping', 'aggressive compression'],
                    'solutions': [
                        'Cut 3-5kHz with narrow Q',
                        'Use analog-modeled plugins for warmth',
                        'Reduce compression ratio',
                        'Add gentle high-shelf cut above 10kHz'
                    ]
                },
                'weak_low_end': {
                    'causes': ['poor monitoring', 'phase cancellation', 'insufficient sub content'],
                    'solutions': [
                        'Check phase relationship between kick and bass',
                        'Boost 50-100Hz on bass elements',
                        'Use harmonic saturation to add presence',
                        'Ensure mono compatibility below 120Hz'
                    ]
                },
                'lack_of_depth': {
                    'causes': ['no reverb', 'flat panning', 'similar dynamics'],
                    'solutions': [
                        'Add reverb with varying decay times',
                        'Pan instruments across stereo field',
                        'Use automation for movement',
                        'Create front-to-back depth with EQ and reverb'
                    ]
                }
            }
        }
        
    def analyze_daw_context(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze DAW context for intelligent suggestions"""
        analysis = {
            'track_count': 0,
            'track_types': {},
            'potential_issues': [],
            'recommendations': []
        }
        
        # Extract track information
        tracks = context.get('tracks', [])
        analysis['track_count'] = len(tracks)
        
        # Analyze track distribution
        for track in tracks:
            track_type = track.get('type', 'audio')
            analysis['track_types'][track_type] = analysis['track_types'].get(track_type, 0) + 1
        
        # Generate intelligent recommendations based on project state
        if analysis['track_count'] == 0:
            analysis['recommendations'].append("Start by adding tracks - audio, instrument, or MIDI")
        elif analysis['track_count'] > 50:
            analysis['potential_issues'].append("High track count may impact performance")
            analysis['recommendations'].append("Consider bouncing similar tracks to audio")
        
        # Check for balance
        if 'audio' in analysis['track_types'] and 'instrument' in analysis['track_types']:
            if analysis['track_types']['instrument'] > analysis['track_types']['audio'] * 3:
                analysis['recommendations'].append("Consider balancing software instruments with recorded audio")
        
        return analysis
        
    def generate_mixing_suggestions(self, track_type: str, track_info: Optional[Dict[str, Any]] = None) -> List[str]:
        """Generate intelligent mixing suggestions based on track type"""
        suggestions = []
        
        # Get track-specific knowledge
        track_knowledge = self.daw_knowledge['track_types'].get(track_type, self.daw_knowledge['track_types']['audio'])
        
        # Add general suggestions
        suggestions.extend(track_knowledge['mixing_tips'][:3])
        
        # Add context-specific suggestions if track info provided
        if track_info:
            peak_level = track_info.get('peak_level', -6)
            if peak_level > -3:
                suggestions.append("?? Peak level is too high - reduce gain for proper headroom")
            elif peak_level < -12:
                suggestions.append("?? Track level is low - increase gain for better signal-to-noise ratio")
            
            # Check if muted/soloed
            if track_info.get('muted'):
                suggestions.append("?? Track is currently muted - unmute to hear changes")
            if track_info.get('soloed'):
                suggestions.append("?? Track is soloed - remember to check in full mix context")
        
        return suggestions
    
    def analyze_frequency_content(self, frequency_data: Optional[List[float]] = None) -> Dict[str, Any]:
        """Analyze frequency content and provide recommendations"""
        if not frequency_data or len(frequency_data) == 0:
            return {
                'analysis': 'No frequency data provided',
                'recommendations': ['Upload audio for detailed frequency analysis']
            }
        
        analysis = {
            'frequency_balance': {},
            'problem_areas': [],
            'recommendations': []
        }
        
        # Simulate frequency analysis (in real implementation, use FFT)
        freq_ranges = self.daw_knowledge['frequency_ranges']
        
        # Example analysis
        analysis['recommendations'] = [
            "Balance low-end frequencies (60-250Hz) for warmth without muddiness",
            "Ensure presence range (2-4kHz) is clear for definition",
            "Add air frequencies (8-12kHz) for openness and sparkle",
            "Check for phase issues in bass frequencies"
        ]
        
        return analysis
    
    def detect_mixing_problems(self, mix_analysis: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Detect common mixing problems and provide solutions"""
        problems = []
        
        # Analyze based on provided metrics
        if 'peak_level' in mix_analysis:
            if mix_analysis['peak_level'] > -1:
                problems.append({
                    'problem': 'Clipping Risk',
                    'severity': 'high',
                    'solution': 'Reduce master fader by 3-6dB to prevent clipping',
                    'technical_detail': f"Current peak: {mix_analysis['peak_level']:.1f}dB, Target: -3dB"
                })
        
        if 'rms_level' in mix_analysis:
            if mix_analysis['rms_level'] > -6:
                problems.append({
                    'problem': 'Over-compression',
                    'severity': 'medium',
                    'solution': 'Mix sounds overly compressed - reduce compression ratios',
                    'technical_detail': f"RMS level: {mix_analysis['rms_level']:.1f}dB, Target: -12 to -18dB"
                })
        
        if 'track_count' in mix_analysis:
            if mix_analysis['track_count'] > 64:
                problems.append({
                    'problem': 'CPU Load',
                    'severity': 'medium',
                    'solution': 'Consider freezing tracks or bouncing to audio',
                    'technical_detail': f"{mix_analysis['track_count']} tracks may cause latency"
                })
        
        return problems

    def get_wordnet_pos(self, treebank_tag):
        if treebank_tag.startswith('J'):
            return wordnet.ADJ
        elif treebank_tag.startswith('V'):
            return wordnet.VERB
        elif treebank_tag.startswith('N'):
            return wordnet.NOUN
        elif treebank_tag.startswith('R'):
            return wordnet.ADV
        else:
            return None

    def generate_creative_sentence(self, seed_words):
        sentence_patterns = [
            "The {noun} {verb} {adverb} through the {adjective} {noun2}",
            "In the realm of {noun}, we find {adjective} {noun2} that {verb} {adverb}",
            "Through {adjective} observation, the {noun} {verb} to {verb2} {adverb}",
            "Like a {adjective} {noun}, thoughts {verb} {adverb} in the {noun2}",
            "{Adverb}, the {adjective} {noun} {verb} beyond {noun2}"
        ]
        
        # Create word pools
        words = {
            'noun': ['pattern', 'system', 'concept', 'insight', 'knowledge', 'wisdom', 'understanding', 'perspective', 'framework', 'structure'],
            'verb': ['emerges', 'flows', 'evolves', 'transforms', 'adapts', 'resonates', 'harmonizes', 'integrates', 'synthesizes', 'manifests'],
            'adjective': ['dynamic', 'profound', 'intricate', 'harmonious', 'quantum', 'resonant', 'synergistic', 'emergent', 'holistic', 'integrated'],
            'adverb': ['naturally', 'seamlessly', 'elegantly', 'precisely', 'harmoniously', 'dynamically', 'quantum-mechanically', 'synergistically', 'holistically', 'adaptively'],
            'noun2': ['consciousness', 'understanding', 'reality', 'dimension', 'paradigm', 'framework', 'ecosystem', 'universe', 'matrix', 'field']
        }
        
        # Add seed words to appropriate categories
        try:
            for word, pos in pos_tag(word_tokenize(' '.join(seed_words))):
                pos_type = self.get_wordnet_pos(pos)
                if pos_type == wordnet.NOUN:
                    words['noun'].append(word)
                    words['noun2'].append(word)
                elif pos_type == wordnet.VERB:
                    words['verb'].append(word)
                elif pos_type == wordnet.ADJ:
                    words['adjective'].append(word)
                elif pos_type == wordnet.ADV:
                    words['adverb'].append(word)
        except Exception as e:
            logger.warning(f"Could not process seed words: {e}")

        # Generate sentence
        pattern = random.choice(sentence_patterns)
        sentence = pattern.format(
            noun=random.choice(words['noun']),
            verb=random.choice(words['verb']),
            adjective=random.choice(words['adjective']),
            adverb=random.choice(words['adverb']),
            noun2=random.choice(words['noun2']),
            verb2=random.choice(words['verb']),
            Adverb=random.choice(words['adverb']).capitalize()
        )
        return sentence

    def audit_log(self, message, system=False):
        source = "SYSTEM" if system else self.user_name
        logging.info(f"{source}: {message}")

    def analyze_sentiment(self, text):
        score = self.analyzer.polarity_scores(text)
        self.audit_log(f"Sentiment analysis: {score}")
        return score

    def extract_key_concepts(self, text):
        try:
            tokens = word_tokenize(text.lower())
            tagged = pos_tag(tokens)
            concepts = []
            for word, tag in tagged:
                if tag.startswith(('NN', 'VB', 'JJ', 'RB')):
                    concepts.append(word)
            return concepts
        except Exception as e:
            logger.warning(f"Could not extract concepts: {e}")
            # Fallback: just split by spaces
            return [w for w in text.lower().split() if len(w) > 2]

    def respond(self, prompt):
        """Generate multi-perspective response with DAW intelligence"""
        # Analyze sentiment and extract concepts
        sentiment = self.analyze_sentiment(prompt)
        key_concepts = self.extract_key_concepts(prompt)
        self.memory.append({"prompt": prompt, "sentiment": sentiment, "concepts": key_concepts})
        
        # Check if this is a DAW-related query
        daw_keywords = ['mix', 'eq', 'compress', 'track', 'audio', 'bass', 'vocal', 'drum', 'frequency', 
                       'gain', 'reverb', 'delay', 'master', 'pan', 'stereo', 'plugin', 'daw', 'recording',
                       'sound', 'volume', 'level', 'effect', 'processing']
        is_daw_query = any(keyword in prompt.lower() for keyword in daw_keywords)
        
        # Generate responses using multiple perspectives
        responses = []
        
        # If DAW query, prioritize DAW-specific response first
        if is_daw_query:
            daw_response = self._generate_daw_specific_response(prompt, key_concepts)
            responses.append(f"[DAW Expert] {daw_response}")
            # Limit other perspectives for DAW queries to keep response focused
            responses.append(f"[Neural] {self.generate_creative_sentence(key_concepts if key_concepts else ['understanding'])}")
        else:
            # For non-DAW queries, use full multi-perspective analysis
            # Neural perspective with creative sentence
            neural_insight = self.generate_creative_sentence(key_concepts if key_concepts else ["understanding"])
            responses.append(f"[Neural] {neural_insight}")
        
        # Define response templates (only use if not DAW query)
        if not is_daw_query:
            response_templates = {
                'logical': [
                    "Following cause and effect: {cause} leads to {effect}.",
                    "Logical analysis shows that {premise} implies {conclusion}.",
                    "Structured reasoning suggests {insight}."
                ],
                'creative': [
                    "Imagine {metaphor} - this illustrates how {concept} relates to {application}.",
                    "Like {natural_process}, we can see how {principle} emerges naturally.",
                    "Visualize {scenario} to understand the deeper patterns."
                ]
            }

            # Define variables for template filling
            variables = {
                'cause': ['careful analysis', 'systematic approach', 'balanced perspective'],
                'effect': ['improved understanding', 'better outcomes', 'sustainable solutions'],
                'premise': ['current conditions', 'observed patterns', 'established principles'],
                'conclusion': ['strategic adaptation', 'systematic improvement', 'harmonious integration'],
                'insight': ['patterns emerge from chaos', 'balance leads to stability', 'adaptation drives growth'],
                'metaphor': ['a river finding its path', 'a tree growing towards light', 'a crystal forming in solution'],
                'concept': ['natural growth', 'adaptive learning', 'emergent behavior'],
                'application': ['our current situation', 'the challenge at hand', 'our approach'],
                'natural_process': ['evolution', 'crystallization', 'metamorphosis'],
                'principle': ['self-organization', 'natural selection', 'emergent complexity'],
                'scenario': ['a garden in bloom', 'a constellation of stars', 'a forest ecosystem']
            }

            # Select random perspectives (1-2 for non-DAW queries)
            perspectives = list(response_templates.keys())
            np.random.shuffle(perspectives)
            num_perspectives = np.random.randint(1, min(3, len(perspectives) + 1))
            selected_perspectives = perspectives[:num_perspectives]

            # Generate responses
            for perspective in selected_perspectives:
                template = np.random.choice(response_templates[perspective])
                # Replace variables in template
                response = template
                for var in re.findall(r'\{(\w+)\}', template):
                    if var in variables:
                        replacement = np.random.choice(variables[var])
                        response = response.replace('{'+var+'}', replacement)
                responses.append(f"[{perspective.capitalize()}] {response}")

        # Save conversation to DB (user prompt + Codette response)
        try:
            full_response = "\n\n".join(responses)
            self.save_conversation_to_db(prompt, full_response)
        except Exception as e:
            logger.warning(f"Could not save conversation to DB: {e}")

        return "\n\n".join(responses)
    
    def _generate_daw_specific_response(self, prompt: str, concepts: List[str]) -> str:
        """Generate DAW-specific intelligent response"""
        prompt_lower = prompt.lower()
        
        # Mix improvement queries
        if any(word in prompt_lower for word in ['mix', 'mixing', 'improve']):
            return ("For better mixing: Start with gain staging (-6dB peaks), use subtractive EQ first, "
                   "apply gentle compression (3:1 to 4:1 ratio), create depth with reverb and delay, "
                   "and always reference on multiple systems. Remember: clarity comes from frequency separation.")
        
        # EQ queries
        if 'eq' in prompt_lower or 'equaliz' in prompt_lower or 'frequency' in prompt_lower:
            return ("EQ fundamentals: Cut before you boost, use narrow Q for problem frequencies, "
                   "wide Q for tonal shaping. Key ranges: 60-250Hz (warmth), 250-500Hz (body), "
                   "2-4kHz (presence), 8-12kHz (air). High-pass non-bass instruments above 80-100Hz.")
        
        # Compression queries
        if 'compress' in prompt_lower:
            return ("Compression guide: Attack 10-30ms to preserve transients, release set to tempo or auto, "
                   "ratio 3:1 for subtle (vocals), 6:1+ for aggressive (drums). Aim for 3-6dB gain reduction. "
                   "Use parallel compression for natural punch without over-compression.")
        
        # Bass/low-end queries
        if any(word in prompt_lower for word in ['bass', 'low end', 'sub']):
            return ("Low-end mastery: Keep sub-bass (20-60Hz) mono, boost kick fundamental at 50-100Hz, "
                   "use sidechain compression between kick and bass, check phase relationship, "
                   "add harmonic saturation for definition on small speakers.")
        
        # Vocal queries
        if 'vocal' in prompt_lower or 'voice' in prompt_lower:
            return ("Vocal mixing: De-ess at 6-8kHz, compress with 3:1 to 6:1 ratio, boost presence at 3-5kHz, "
                   "add air at 10-12kHz, use double-tracking or subtle delay for width, "
                   "apply reverb via send for control. Keep vocals centered and upfront.")
        
        # General improvement
        return ("Key mixing principles: Proper gain staging prevents problems, EQ creates space, "
               "compression controls dynamics, reverb/delay add depth, automation adds life. "
               "Trust your ears, take breaks, and reference professional tracks.")
