{
  "version": "2.0",
  "description": "Codette v3 Enhanced Model Configuration",
  "last_updated": "2025-12-02",
  "author": "jonathan.harrison1",
  
  "models": {
    "codette_v3": {
      "path": "${CODETTE_MODEL_ID}",
      "description": "Codette v3 Quantum Multicore - Custom audio analysis engine",
      "type": "custom",
      "version": "3.0",
      "capabilities": ["audio_analysis", "cognitive_reasoning", "real_time_processing"],
      "performance": {
        "load_time_seconds": 4.0,
        "inference_time_ms": 45,
        "memory_mb": 500
      },
      "requirements": {
        "min_ram_gb": 4,
        "recommended_ram_gb": 8,
        "gpu_optional": true,
        "gpu_vram_mb": 2048
      },
      "optimization": {
        "cache": true,
        "quantize": false,
        "use_fp16": false,
        "load_in_8bit": false
      },
      "cache_settings": {
        "enabled": true,
        "max_entries": 500,
        "ttl_hours": 48
      },
      "ensemble": {
        "priority": 1,
        "weight": 1.0,
        "fallback_on_error": true
      },
      "ab_test_ready": true
    },
    
    "gpt2_large": {
      "path": "gpt2-large",
      "description": "GPT-2 Large - Fallback general-purpose model",
      "type": "transformer",
      "version": "1.0",
      "capabilities": ["text_generation", "analysis"],
      "performance": {
        "load_time_seconds": 2.5,
        "inference_time_ms": 60,
        "memory_mb": 1500
      },
      "requirements": {
        "min_ram_gb": 4,
        "recommended_ram_gb": 8,
        "gpu_optional": true,
        "gpu_vram_mb": 4096
      },
      "optimization": {
        "cache": true,
        "quantize": true,
        "use_fp16": true,
        "load_in_8bit": true
      },
      "cache_settings": {
        "enabled": true,
        "max_entries": 300,
        "ttl_hours": 24
      },
      "ensemble": {
        "priority": 2,
        "weight": 0.5,
        "fallback_on_error": true
      },
      "ab_test_ready": true
    },
    
    "phi_2": {
      "path": "microsoft/phi-2",
      "description": "Microsoft Phi-2 - Efficient reasoning model",
      "type": "transformer",
      "version": "2.0",
      "capabilities": ["reasoning", "code_understanding", "analysis"],
      "performance": {
        "load_time_seconds": 3.0,
        "inference_time_ms": 50,
        "memory_mb": 1200
      },
      "requirements": {
        "min_ram_gb": 6,
        "recommended_ram_gb": 12,
        "gpu_optional": true,
        "gpu_vram_mb": 6144
      },
      "optimization": {
        "cache": true,
        "quantize": true,
        "use_fp16": true,
        "load_in_8bit": true
      },
      "cache_settings": {
        "enabled": true,
        "max_entries": 400,
        "ttl_hours": 36
      },
      "ensemble": {
        "priority": 2,
        "weight": 0.75,
        "fallback_on_error": true
      },
      "ab_test_ready": true
    },
    
    "mistral_7b": {
      "path": "mistralai/Mistral-7B-Instruct-v0.1",
      "description": "Mistral 7B - Advanced instruction-following model",
      "type": "transformer",
      "version": "7.0",
      "capabilities": ["instruction_following", "reasoning", "coding"],
      "performance": {
        "load_time_seconds": 5.0,
        "inference_time_ms": 100,
        "memory_mb": 2500
      },
      "requirements": {
        "min_ram_gb": 16,
        "recommended_ram_gb": 24,
        "gpu_optional": false,
        "gpu_vram_mb": 8192
      },
      "optimization": {
        "cache": true,
        "quantize": true,
        "use_fp16": true,
        "load_in_8bit": true
      },
      "cache_settings": {
        "enabled": true,
        "max_entries": 200,
        "ttl_hours": 24
      },
      "ensemble": {
        "priority": 3,
        "weight": 1.0,
        "fallback_on_error": false
      },
      "ab_test_ready": true
    }
  },
  
  "default_model": "codette_v3",
  "fallback_models": ["gpt2_large", "phi_2"],
  
  "caching": {
    "global_enabled": true,
    "cache_size": 1000,
    "cache_ttl_hours": 24,
    "cache_path": "./model_cache",
    "compression": true,
    "compression_algorithm": "gzip"
  },
  
  "ensemble": {
    "enabled": false,
    "default_models": ["codette_v3", "phi_2"],
    "voting_strategy": "weighted_average",
    "consensus_threshold": 0.7,
    "parallel_processing": true,
    "timeout_seconds": 30
  },
  
  "ab_testing": {
    "enabled": true,
    "results_file": "./ab_test_results.jsonl",
    "min_samples_for_conclusion": 30,
    "confidence_threshold": 0.85,
    "auto_save_interval": 10
  },
  
  "fine_tuning": {
    "enabled": true,
    "checkpoint_dir": "./model_checkpoints",
    "learning_rate": 5e-5,
    "batch_size": 8,
    "epochs": 3,
    "warmup_steps": 100,
    "save_strategy": "epoch",
    "eval_strategy": "epoch"
  },
  
  "monitoring": {
    "enabled": true,
    "metrics_path": "./metrics",
    "log_level": "INFO",
    "track_metrics": [
      "load_time",
      "inference_time",
      "memory_usage",
      "error_rate",
      "cache_hit_rate",
      "throughput"
    ],
    "alert_thresholds": {
      "error_rate_percent": 5,
      "memory_usage_percent": 85,
      "inference_time_ms": 500
    }
  },
  
  "hardware": {
    "device_map": "auto",
    "torch_dtype": "float16",
    "load_in_8bit": true,
    "load_in_4bit": false,
    "max_memory": null,
    "gpu_memory_fraction": 0.9
  },
  
  "security": {
    "validate_inputs": true,
    "max_input_length": 4096,
    "sanitize_outputs": true,
    "require_authentication": false,
    "rate_limiting": {
      "enabled": true,
      "requests_per_minute": 100
    }
  },
  
  "performance": {
    "batch_processing": true,
    "max_batch_size": 32,
    "prefetch_enabled": true,
    "async_enabled": true,
    "thread_pool_size": 4
  },
  
  "optimization": {
    "aggressive_caching": true,
    "memory_efficient": true,
    "cpu_offload": true,
    "gradient_checkpointing": false,
    "attention_optimization": "flash_attention"
  }
}
